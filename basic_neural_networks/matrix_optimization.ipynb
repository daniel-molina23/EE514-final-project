{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the trianable ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pennylane as qml\n",
    "\n",
    "I = np.eye(2)\n",
    "n_qubits = 6\n",
    "size_of_vec = 2**n_qubits\n",
    "num_layers = 5\n",
    "\n",
    "from utils import DFS\n",
    "inputStates, expectedStates = DFS().getInitialTargetStates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_half_matrix(num_layers, addHalf=False):\n",
    "    import numpy as np\n",
    "    from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "    from utils import MatrixUtils\n",
    "    matrixUtils = MatrixUtils()\n",
    "    # Define the correct operations you want the matrix to perform on basis vectors\n",
    "    def target_operations(parameters, inputStates):\n",
    "        # Reshape the parameters into the matrix form\n",
    "        normalParams = parameters[:num_layers*5]\n",
    "        halfParams = parameters[num_layers*5:] if addHalf else None\n",
    "\n",
    "        parameters = np.reshape(normalParams, (num_layers, 5))\n",
    "        matrix = matrixUtils.get_total_matrix(size_of_vec=2**6, weights=parameters, halfWeights=halfParams)\n",
    "\n",
    "        # Perform matrix multiplication with basis vectors\n",
    "        results = []\n",
    "        for i in range(len(inputStates)):\n",
    "            results.append(np.matmul(matrix, inputStates[i]))\n",
    "\n",
    "        # Define the target operations you want (modify this based on your specific task)\n",
    "        target_result = np.array(expectedStates)\n",
    "\n",
    "        # Calculate the loss as the difference between the obtained result and the target result\n",
    "        # loss = square_loss(target_result, results)\n",
    "        loss = matrixUtils.f_cnot_loss(target_result, results)\n",
    "        return loss\n",
    "\n",
    "    # Generate random basis vectors and target result\n",
    "    basis_vectors = np.array(inputStates)\n",
    "    target_result = np.array(expectedStates)\n",
    "\n",
    "    # Flatten the matrix parameters for optimization\n",
    "    initial_parameters = np.ndarray.flatten(matrixUtils.get_random_weights(num_layers))\n",
    "\n",
    "    initial_parameters = np.concatenate((initial_parameters, matrixUtils.get_random_half_layer_weights())) if addHalf else initial_parameters\n",
    "\n",
    "    # Use scipy's minimize function to optimize the parameters\n",
    "    result = minimize(target_operations, initial_parameters, args=(basis_vectors,), method='L-BFGS-B')\n",
    "\n",
    "    firstHalf, secondHalf = result.x[:num_layers*5], result.x[num_layers*5:] if addHalf else None\n",
    "    # Reshape the optimized parameters back into the matrix form\n",
    "    optimized_matrix = matrixUtils.get_total_matrix(size_of_vec=2**6, weights=firstHalf.reshape((num_layers, 5)), halfWeights=secondHalf)\n",
    "\n",
    "    # print(\"Optimized Matrix:\")\n",
    "    # print(optimized_matrix)\n",
    "    total_layers = num_layers + 0.5 if addHalf else num_layers\n",
    "\n",
    "    predStates = [np.matmul(optimized_matrix, mat) for mat in inputStates]\n",
    "    fcnot_loss = matrixUtils.f_cnot_loss(expectedStates, predStates)\n",
    "    # print(f\"f_cnot_loss for {total_layers} layers = {fcnot_loss}\")\n",
    "    # print(f\"square_loss for {total_layers} layers = {matrixUtils.square_loss(expectedStates, predStates)}\")\n",
    "    return optimized_matrix, result.x, fcnot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model with layers: 3, itertation: 1, fcnot_loss: 1.0745380149674383e-07\n",
      "Trained model with layers: 3, itertation: 2, fcnot_loss: 0.31138335934926215\n",
      "Trained model with layers: 3, itertation: 3, fcnot_loss: 5.5755039852469285e-08\n",
      "Trained model with layers: 3, itertation: 4, fcnot_loss: 4.8801246035156886e-06\n",
      "Trained model with layers: 3, itertation: 5, fcnot_loss: 1.3818768405162847e-07\n",
      "Trained model with layers: 3, itertation: 6, fcnot_loss: 6.989264130329236e-08\n",
      "Trained model with layers: 3, itertation: 7, fcnot_loss: 0.3108295966931766\n",
      "Trained model with layers: 3, itertation: 8, fcnot_loss: 4.470348358154297e-08\n",
      "Trained model with layers: 3, itertation: 9, fcnot_loss: 0.32099550455774634\n",
      "Trained model with layers: 3, itertation: 10, fcnot_loss: 0.22960900937847328\n",
      "Trained model with layers: 3, itertation: 11, fcnot_loss: 6.363162241219334e-07\n",
      "Trained model with layers: 3, itertation: 12, fcnot_loss: 0.30939020319424504\n",
      "Trained model with layers: 3, itertation: 13, fcnot_loss: 0.5170438950002937\n",
      "Trained model with layers: 3, itertation: 14, fcnot_loss: 6.926844490028643e-06\n",
      "Trained model with layers: 3, itertation: 15, fcnot_loss: 1.3995593886456422e-06\n",
      "Trained model with layers: 3, itertation: 16, fcnot_loss: 0.5791092569746561\n",
      "Trained model with layers: 3, itertation: 17, fcnot_loss: 0.05198494584417139\n",
      "Trained model with layers: 3, itertation: 18, fcnot_loss: 2.862070495338878e-06\n",
      "Trained model with layers: 3, itertation: 19, fcnot_loss: 0.31495595543853155\n",
      "Trained model with layers: 3, itertation: 20, fcnot_loss: 7.524717553229688e-08\n",
      "Trained model with layers: 3.5, itertation: 1, fcnot_loss: 1.5910086411284374e-07\n",
      "Trained model with layers: 3.5, itertation: 2, fcnot_loss: 4.470348358154297e-08\n",
      "Trained model with layers: 3.5, itertation: 3, fcnot_loss: 0.31678957423593934\n",
      "Trained model with layers: 3.5, itertation: 4, fcnot_loss: 3.635425274949657e-06\n",
      "Trained model with layers: 3.5, itertation: 5, fcnot_loss: 1.1496127558938211e-06\n",
      "Trained model with layers: 3.5, itertation: 6, fcnot_loss: 6.495265578539184e-08\n",
      "Trained model with layers: 3.5, itertation: 7, fcnot_loss: 5.960464477539063e-08\n",
      "Trained model with layers: 3.5, itertation: 8, fcnot_loss: 4.169676676666e-06\n",
      "Trained model with layers: 3.5, itertation: 9, fcnot_loss: 3.686208162962281e-06\n",
      "Trained model with layers: 3.5, itertation: 10, fcnot_loss: 1.251165242772301e-07\n",
      "Trained model with layers: 3.5, itertation: 11, fcnot_loss: 4.564233214250389e-06\n",
      "Trained model with layers: 3.5, itertation: 12, fcnot_loss: 5.279744477424391e-06\n",
      "Trained model with layers: 3.5, itertation: 13, fcnot_loss: 4.969040155569632e-07\n",
      "Trained model with layers: 3.5, itertation: 14, fcnot_loss: 4.9421560620597e-08\n",
      "Trained model with layers: 3.5, itertation: 15, fcnot_loss: 0.3219265074976691\n",
      "Trained model with layers: 3.5, itertation: 16, fcnot_loss: 0.21634124525094894\n",
      "Trained model with layers: 3.5, itertation: 17, fcnot_loss: 0.32260784007596766\n",
      "Trained model with layers: 3.5, itertation: 18, fcnot_loss: 7.450580596923828e-08\n",
      "Trained model with layers: 3.5, itertation: 19, fcnot_loss: 6.074850890230986e-06\n",
      "Trained model with layers: 3.5, itertation: 20, fcnot_loss: 2.570180343189886e-06\n",
      "Trained model with layers: 4, itertation: 1, fcnot_loss: 7.884953353001448e-08\n",
      "Trained model with layers: 4, itertation: 2, fcnot_loss: 1.8789649326977866e-07\n",
      "Trained model with layers: 4, itertation: 3, fcnot_loss: 8.93131648170038e-06\n",
      "Trained model with layers: 4, itertation: 4, fcnot_loss: 2.9802322387695312e-08\n",
      "Trained model with layers: 4, itertation: 5, fcnot_loss: 7.524717553229688e-08\n",
      "Trained model with layers: 4, itertation: 6, fcnot_loss: 8.161702118890973e-08\n",
      "Trained model with layers: 4, itertation: 7, fcnot_loss: 8.363255977870393e-08\n",
      "Trained model with layers: 4, itertation: 8, fcnot_loss: 1.0269916617937551e-07\n",
      "Trained model with layers: 4, itertation: 9, fcnot_loss: 0.044581015323020584\n",
      "Trained model with layers: 4, itertation: 10, fcnot_loss: 4.2146848510894035e-08\n",
      "Trained model with layers: 4, itertation: 11, fcnot_loss: 0.029362227223599657\n",
      "Trained model with layers: 4, itertation: 12, fcnot_loss: 1.0536712127723509e-08\n",
      "Trained model with layers: 4, itertation: 13, fcnot_loss: 1.8341144360737757e-07\n",
      "Trained model with layers: 4, itertation: 14, fcnot_loss: 3.942476676500724e-08\n",
      "Trained model with layers: 4, itertation: 15, fcnot_loss: 7.598131170262075e-08\n",
      "Trained model with layers: 4, itertation: 16, fcnot_loss: 1.0624298037038725e-05\n",
      "Trained model with layers: 4, itertation: 17, fcnot_loss: 1.666000468656264e-07\n",
      "Trained model with layers: 4, itertation: 18, fcnot_loss: 6.495265578539184e-08\n",
      "Trained model with layers: 4, itertation: 19, fcnot_loss: 2.2376563277365576e-07\n",
      "Trained model with layers: 4, itertation: 20, fcnot_loss: 5.7711949142924205e-08\n",
      "Trained model with layers: 4.5, itertation: 1, fcnot_loss: 9.541398640621973e-08\n",
      "Trained model with layers: 4.5, itertation: 2, fcnot_loss: 4.59284633619262e-08\n",
      "Trained model with layers: 4.5, itertation: 3, fcnot_loss: 8.093402155637902e-08\n",
      "Trained model with layers: 4.5, itertation: 4, fcnot_loss: 9.771344846937061e-08\n",
      "Trained model with layers: 4.5, itertation: 5, fcnot_loss: 8.296615425989064e-08\n",
      "Trained model with layers: 4.5, itertation: 6, fcnot_loss: 1.0536712127723508e-07\n",
      "Trained model with layers: 4.5, itertation: 7, fcnot_loss: 1.7249341164216735e-07\n",
      "Trained model with layers: 4.5, itertation: 8, fcnot_loss: 1.043081283569336e-07\n",
      "Trained model with layers: 4.5, itertation: 9, fcnot_loss: 0.07407483135233599\n",
      "Trained model with layers: 4.5, itertation: 10, fcnot_loss: 0.2824626846380574\n",
      "Trained model with layers: 4.5, itertation: 11, fcnot_loss: 6.069987600780594e-06\n",
      "Trained model with layers: 4.5, itertation: 12, fcnot_loss: 1.666000468656264e-07\n",
      "Trained model with layers: 4.5, itertation: 13, fcnot_loss: 0.007481952621572392\n",
      "Trained model with layers: 4.5, itertation: 14, fcnot_loss: 8.752450439684637e-08\n",
      "Trained model with layers: 4.5, itertation: 15, fcnot_loss: 0.08242214533362309\n",
      "Trained model with layers: 4.5, itertation: 16, fcnot_loss: 6.746787680787904e-08\n",
      "Trained model with layers: 4.5, itertation: 17, fcnot_loss: 1.4632758199642599e-06\n",
      "Trained model with layers: 4.5, itertation: 18, fcnot_loss: 1.744136200952474e-07\n",
      "Trained model with layers: 4.5, itertation: 19, fcnot_loss: 1.021572142899192e-07\n",
      "Trained model with layers: 4.5, itertation: 20, fcnot_loss: 6.143906154658885e-08\n",
      "Trained model with layers: 5, itertation: 1, fcnot_loss: 2.491213902428819e-07\n",
      "Trained model with layers: 5, itertation: 2, fcnot_loss: 6.507304847802053e-06\n",
      "Trained model with layers: 5, itertation: 3, fcnot_loss: 8.024520884654677e-08\n",
      "Trained model with layers: 5, itertation: 4, fcnot_loss: 5.5755039852469285e-08\n",
      "Trained model with layers: 5, itertation: 5, fcnot_loss: 8.462192195291202e-05\n",
      "Trained model with layers: 5, itertation: 6, fcnot_loss: 0.00022555601280393236\n",
      "Trained model with layers: 5, itertation: 7, fcnot_loss: 4.3443977049330625e-07\n",
      "Trained model with layers: 5, itertation: 8, fcnot_loss: 1.3575603930867164e-07\n",
      "Trained model with layers: 5, itertation: 9, fcnot_loss: 2.2619354072746133e-06\n",
      "Trained model with layers: 5, itertation: 10, fcnot_loss: 1.71525246399959e-07\n",
      "Trained model with layers: 5, itertation: 11, fcnot_loss: 5.736463235294865e-07\n",
      "Trained model with layers: 5, itertation: 12, fcnot_loss: 2.1335215960854819e-07\n",
      "Trained model with layers: 5, itertation: 13, fcnot_loss: 5.5755039852469285e-08\n",
      "Trained model with layers: 5, itertation: 14, fcnot_loss: 0.00010055283257837581\n",
      "Trained model with layers: 5, itertation: 15, fcnot_loss: 2.0185436582779982e-07\n",
      "Trained model with layers: 5, itertation: 16, fcnot_loss: 7.0345949981080825e-06\n",
      "Trained model with layers: 5, itertation: 17, fcnot_loss: 2.172635803817341e-05\n",
      "Trained model with layers: 5, itertation: 18, fcnot_loss: 7.849673742691232e-07\n",
      "Trained model with layers: 5, itertation: 19, fcnot_loss: 1.9542689693874122e-07\n",
      "Trained model with layers: 5, itertation: 20, fcnot_loss: 1.3328003749250113e-07\n",
      "Trained model with layers: 5.5, itertation: 1, fcnot_loss: 9.771344846937061e-08\n",
      "Trained model with layers: 5.5, itertation: 2, fcnot_loss: 4.344397704933063e-08\n",
      "Trained model with layers: 5.5, itertation: 3, fcnot_loss: 1.043081283569336e-07\n",
      "Trained model with layers: 5.5, itertation: 4, fcnot_loss: 7.146345858741878e-08\n",
      "Trained model with layers: 5.5, itertation: 5, fcnot_loss: 1.1827430029502171e-07\n",
      "Trained model with layers: 5.5, itertation: 6, fcnot_loss: 6.746787680787904e-08\n",
      "Trained model with layers: 5.5, itertation: 7, fcnot_loss: 9.353372351214418e-07\n",
      "Trained model with layers: 5.5, itertation: 8, fcnot_loss: 4.275049396459587e-06\n",
      "Trained model with layers: 5.5, itertation: 9, fcnot_loss: 7.231286501020746e-07\n",
      "Trained model with layers: 5.5, itertation: 10, fcnot_loss: 4.712160915387242e-08\n",
      "Trained model with layers: 5.5, itertation: 11, fcnot_loss: 1.1827430029502171e-07\n",
      "Trained model with layers: 5.5, itertation: 12, fcnot_loss: 1.8671101460071903e-07\n",
      "Trained model with layers: 5.5, itertation: 13, fcnot_loss: 9.657056180250437e-08\n",
      "Trained model with layers: 5.5, itertation: 14, fcnot_loss: 7.203759826599685e-06\n",
      "Trained model with layers: 5.5, itertation: 15, fcnot_loss: 1.3493575361575808e-07\n",
      "Trained model with layers: 5.5, itertation: 16, fcnot_loss: 8.161702118890973e-08\n",
      "Trained model with layers: 5.5, itertation: 17, fcnot_loss: 1.3878893859831361e-06\n",
      "Trained model with layers: 5.5, itertation: 18, fcnot_loss: 4.470348358154297e-08\n",
      "Trained model with layers: 5.5, itertation: 19, fcnot_loss: 2.517811246024937e-07\n",
      "Trained model with layers: 5.5, itertation: 20, fcnot_loss: 6.495265578539184e-08\n",
      "Trained model with layers: 6, itertation: 1, fcnot_loss: 1.999200562387517e-07\n",
      "Trained model with layers: 6, itertation: 2, fcnot_loss: 8.688795409866125e-08\n",
      "Trained model with layers: 6, itertation: 3, fcnot_loss: 9.541398640621973e-08\n",
      "Trained model with layers: 6, itertation: 4, fcnot_loss: 1.4788980216740972e-07\n",
      "Trained model with layers: 6, itertation: 5, fcnot_loss: 1.965654641665333e-06\n",
      "Trained model with layers: 6, itertation: 6, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 6, itertation: 7, fcnot_loss: 6.664001874625056e-08\n",
      "Trained model with layers: 6, itertation: 8, fcnot_loss: 1.5619942762111018e-06\n",
      "Trained model with layers: 6, itertation: 9, fcnot_loss: 1.1967404487884797e-07\n",
      "Trained model with layers: 6, itertation: 10, fcnot_loss: 1.3978528260658472e-07\n",
      "Trained model with layers: 6, itertation: 11, fcnot_loss: 2.0185436582779982e-07\n",
      "Trained model with layers: 6, itertation: 12, fcnot_loss: 9.657056180250437e-08\n",
      "Trained model with layers: 6, itertation: 13, fcnot_loss: 1.7654574815986548e-06\n",
      "Trained model with layers: 6, itertation: 14, fcnot_loss: 1.1348386266624791e-07\n",
      "Trained model with layers: 6, itertation: 15, fcnot_loss: 8.528881085666528e-07\n",
      "Trained model with layers: 6, itertation: 16, fcnot_loss: 2.2940046069533605e-07\n",
      "Trained model with layers: 6, itertation: 17, fcnot_loss: 1.666000468656264e-07\n",
      "Trained model with layers: 6, itertation: 18, fcnot_loss: 4.982427804857638e-07\n",
      "Trained model with layers: 6, itertation: 19, fcnot_loss: 1.812804499357408e-07\n",
      "Trained model with layers: 6, itertation: 20, fcnot_loss: 1.5413880993110838e-07\n",
      "Trained model with layers: 6.5, itertation: 1, fcnot_loss: 1.3328003749250113e-07\n",
      "Trained model with layers: 6.5, itertation: 2, fcnot_loss: 1.29477286316326e-07\n",
      "Trained model with layers: 6.5, itertation: 3, fcnot_loss: 6.246946399287755e-07\n",
      "Trained model with layers: 6.5, itertation: 4, fcnot_loss: 6.322027276634104e-08\n",
      "Trained model with layers: 6.5, itertation: 5, fcnot_loss: 1.4975481660844222e-07\n",
      "Trained model with layers: 6.5, itertation: 6, fcnot_loss: 8.940696716308594e-08\n",
      "Trained model with layers: 6.5, itertation: 7, fcnot_loss: 6.580174614733344e-08\n",
      "Trained model with layers: 6.5, itertation: 8, fcnot_loss: 8.229435248067425e-08\n",
      "Trained model with layers: 6.5, itertation: 9, fcnot_loss: 1.0536712127723508e-07\n",
      "Trained model with layers: 6.5, itertation: 10, fcnot_loss: 8.093402155637902e-08\n",
      "Trained model with layers: 6.5, itertation: 11, fcnot_loss: 1.2731557704981462e-07\n",
      "Trained model with layers: 6.5, itertation: 12, fcnot_loss: 3.494632065164618e-08\n",
      "Trained model with layers: 6.5, itertation: 13, fcnot_loss: 9.18569267238524e-08\n",
      "Trained model with layers: 6.5, itertation: 14, fcnot_loss: 6.828569911354542e-08\n",
      "Trained model with layers: 6.5, itertation: 15, fcnot_loss: 1.2377834115800373e-07\n",
      "Trained model with layers: 6.5, itertation: 16, fcnot_loss: 2.8565957416118404e-07\n",
      "Trained model with layers: 6.5, itertation: 17, fcnot_loss: 1.3858881024821308e-07\n",
      "Trained model with layers: 6.5, itertation: 18, fcnot_loss: 1.4971403622959767e-06\n",
      "Trained model with layers: 6.5, itertation: 19, fcnot_loss: 1.3738191809196602e-07\n",
      "Trained model with layers: 6.5, itertation: 20, fcnot_loss: 6.746787680787904e-08\n",
      "Trained model with layers: 7, itertation: 1, fcnot_loss: 1.737759081973225e-07\n",
      "Trained model with layers: 7, itertation: 2, fcnot_loss: 1.187427151986154e-07\n",
      "Trained model with layers: 7, itertation: 3, fcnot_loss: 7.068241373080863e-08\n",
      "Trained model with layers: 7, itertation: 4, fcnot_loss: 9.424321830774485e-08\n",
      "Trained model with layers: 7, itertation: 5, fcnot_loss: 5.335364875187921e-07\n",
      "Trained model with layers: 7, itertation: 6, fcnot_loss: 1.1397196755393114e-07\n",
      "Trained model with layers: 7, itertation: 7, fcnot_loss: 4.712160915387242e-08\n",
      "Trained model with layers: 7, itertation: 8, fcnot_loss: 1.021572142899192e-07\n",
      "Trained model with layers: 7, itertation: 9, fcnot_loss: 5.1619136559035694e-08\n",
      "Trained model with layers: 7, itertation: 10, fcnot_loss: 2.700771236478069e-07\n",
      "Trained model with layers: 7, itertation: 11, fcnot_loss: 1.1348386266624791e-07\n",
      "Trained model with layers: 7, itertation: 12, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 7, itertation: 13, fcnot_loss: 1.021572142899192e-07\n",
      "Trained model with layers: 7, itertation: 14, fcnot_loss: 9.06402249678704e-08\n",
      "Trained model with layers: 7, itertation: 15, fcnot_loss: 1.1348386266624791e-07\n",
      "Trained model with layers: 7, itertation: 16, fcnot_loss: 3.669741855867133e-07\n",
      "Trained model with layers: 7, itertation: 17, fcnot_loss: 1.29477286316326e-07\n",
      "Trained model with layers: 7, itertation: 18, fcnot_loss: 5.674193133312396e-08\n",
      "Trained model with layers: 7, itertation: 19, fcnot_loss: 7.884953353001448e-08\n",
      "Trained model with layers: 7, itertation: 20, fcnot_loss: 2.9802322387695312e-08\n",
      "Trained model with layers: 7.5, itertation: 1, fcnot_loss: 9.771344846937061e-08\n",
      "Trained model with layers: 7.5, itertation: 2, fcnot_loss: 5.960464477539063e-08\n",
      "Trained model with layers: 7.5, itertation: 3, fcnot_loss: 2.908590629092326e-07\n",
      "Trained model with layers: 7.5, itertation: 4, fcnot_loss: 8.024520884654677e-08\n",
      "Trained model with layers: 7.5, itertation: 5, fcnot_loss: 8.296615425989064e-08\n",
      "Trained model with layers: 7.5, itertation: 6, fcnot_loss: 1.3493575361575808e-07\n",
      "Trained model with layers: 7.5, itertation: 7, fcnot_loss: 5.960464477539063e-08\n",
      "Trained model with layers: 7.5, itertation: 8, fcnot_loss: 6.05288029062447e-08\n",
      "Trained model with layers: 7.5, itertation: 9, fcnot_loss: 1.228781230931777e-07\n",
      "Trained model with layers: 7.5, itertation: 10, fcnot_loss: 3.7990655851310376e-08\n",
      "Trained model with layers: 7.5, itertation: 11, fcnot_loss: 3.20807846510574e-07\n",
      "Trained model with layers: 7.5, itertation: 12, fcnot_loss: 8.560065398421926e-08\n",
      "Trained model with layers: 7.5, itertation: 13, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 7.5, itertation: 14, fcnot_loss: 2.043144285798384e-07\n",
      "Trained model with layers: 7.5, itertation: 15, fcnot_loss: 1.377853900857786e-07\n",
      "Trained model with layers: 7.5, itertation: 16, fcnot_loss: 9.06402249678704e-08\n",
      "Trained model with layers: 7.5, itertation: 17, fcnot_loss: 0.00010102813660837276\n",
      "Trained model with layers: 7.5, itertation: 18, fcnot_loss: 3.8078225669803136e-07\n",
      "Trained model with layers: 7.5, itertation: 19, fcnot_loss: 8.267123405146717e-07\n",
      "Trained model with layers: 7.5, itertation: 20, fcnot_loss: 8.024520884654677e-08\n",
      "Trained model with layers: 8, itertation: 1, fcnot_loss: 7.742870483855355e-08\n",
      "Trained model with layers: 8, itertation: 2, fcnot_loss: 2.9802322387695312e-08\n",
      "Trained model with layers: 8, itertation: 3, fcnot_loss: 5.1619136559035694e-08\n",
      "Trained model with layers: 8, itertation: 4, fcnot_loss: 4.0808510594454864e-08\n",
      "Trained model with layers: 8, itertation: 5, fcnot_loss: 8.296615425989064e-08\n",
      "Trained model with layers: 8, itertation: 6, fcnot_loss: 1.4975481660844222e-07\n",
      "Trained model with layers: 8, itertation: 7, fcnot_loss: 9.125060374972142e-08\n",
      "Trained model with layers: 8, itertation: 8, fcnot_loss: 1.1827430029502171e-07\n",
      "Trained model with layers: 8, itertation: 9, fcnot_loss: 6.322027276634104e-08\n",
      "Trained model with layers: 8, itertation: 10, fcnot_loss: 7.955043205642187e-08\n",
      "Trained model with layers: 8, itertation: 11, fcnot_loss: 9.245927368097646e-08\n",
      "Trained model with layers: 8, itertation: 12, fcnot_loss: 1.0950072449966572e-07\n",
      "Trained model with layers: 8, itertation: 13, fcnot_loss: 7.146345858741878e-08\n",
      "Trained model with layers: 8, itertation: 14, fcnot_loss: 8.429369702178807e-08\n",
      "Trained model with layers: 8, itertation: 15, fcnot_loss: 8.940696716308594e-08\n",
      "Trained model with layers: 8, itertation: 16, fcnot_loss: 1.5123026915031998e-07\n",
      "Trained model with layers: 8, itertation: 17, fcnot_loss: 8.429369702178807e-08\n",
      "Trained model with layers: 8, itertation: 18, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 8, itertation: 19, fcnot_loss: 6.143906154658885e-08\n",
      "Trained model with layers: 8, itertation: 20, fcnot_loss: 6.322027276634104e-08\n",
      "Trained model with layers: 8.5, itertation: 1, fcnot_loss: 6.580174614733344e-08\n",
      "Trained model with layers: 8.5, itertation: 2, fcnot_loss: 5.1619136559035694e-08\n",
      "Trained model with layers: 8.5, itertation: 3, fcnot_loss: 2.9802322387695312e-08\n",
      "Trained model with layers: 8.5, itertation: 4, fcnot_loss: 1.4975481660844222e-07\n",
      "Trained model with layers: 8.5, itertation: 5, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 8.5, itertation: 6, fcnot_loss: 1.9852686814180357e-07\n",
      "Trained model with layers: 8.5, itertation: 7, fcnot_loss: 1.1920928955078125e-07\n",
      "Trained model with layers: 8.5, itertation: 8, fcnot_loss: 1.1445799094242763e-07\n",
      "Trained model with layers: 8.5, itertation: 9, fcnot_loss: 3.516800320507868e-07\n",
      "Trained model with layers: 8.5, itertation: 10, fcnot_loss: 8.560065398421926e-08\n",
      "Trained model with layers: 8.5, itertation: 11, fcnot_loss: 3.942476676500724e-08\n",
      "Trained model with layers: 8.5, itertation: 12, fcnot_loss: 1.5699347485382465e-07\n",
      "Trained model with layers: 8.5, itertation: 13, fcnot_loss: 1.8005141576520461e-07\n",
      "Trained model with layers: 8.5, itertation: 14, fcnot_loss: 7.955043205642187e-08\n",
      "Trained model with layers: 8.5, itertation: 15, fcnot_loss: 2.9802322387695312e-08\n",
      "Trained model with layers: 8.5, itertation: 16, fcnot_loss: 4.2146848510894035e-08\n",
      "Trained model with layers: 8.5, itertation: 17, fcnot_loss: 2.9802322387695312e-08\n",
      "Trained model with layers: 8.5, itertation: 18, fcnot_loss: 1.0483896195493853e-07\n",
      "Trained model with layers: 8.5, itertation: 19, fcnot_loss: 1.1250129990683123e-07\n",
      "Trained model with layers: 8.5, itertation: 20, fcnot_loss: 6.143906154658885e-08\n",
      "Trained model with layers: 9, itertation: 1, fcnot_loss: 1.698993779866603e-07\n",
      "Trained model with layers: 9, itertation: 2, fcnot_loss: 6.495265578539184e-08\n",
      "Trained model with layers: 9, itertation: 3, fcnot_loss: 6.495265578539184e-08\n",
      "Trained model with layers: 9, itertation: 4, fcnot_loss: 2.5809568279517847e-08\n",
      "Trained model with layers: 9, itertation: 5, fcnot_loss: 6.233602959916559e-08\n",
      "Trained model with layers: 9, itertation: 6, fcnot_loss: 2.9802322387695312e-08\n",
      "Trained model with layers: 9, itertation: 7, fcnot_loss: 3.650024149988857e-08\n",
      "Trained model with layers: 9, itertation: 8, fcnot_loss: 5.1619136559035694e-08\n",
      "Trained model with layers: 9, itertation: 9, fcnot_loss: 6.322027276634104e-08\n",
      "Trained model with layers: 9, itertation: 10, fcnot_loss: 7.450580596923828e-08\n",
      "Trained model with layers: 9, itertation: 11, fcnot_loss: 7.450580596923828e-08\n",
      "Trained model with layers: 9, itertation: 12, fcnot_loss: 1.0745380149674383e-07\n",
      "Trained model with layers: 9, itertation: 13, fcnot_loss: 1.4600096599955427e-07\n",
      "Trained model with layers: 9, itertation: 14, fcnot_loss: 3.942476676500724e-08\n",
      "Trained model with layers: 9, itertation: 15, fcnot_loss: 6.828569911354542e-08\n",
      "Trained model with layers: 9, itertation: 16, fcnot_loss: 6.746787680787904e-08\n",
      "Trained model with layers: 9, itertation: 17, fcnot_loss: 4.0808510594454864e-08\n",
      "Trained model with layers: 9, itertation: 18, fcnot_loss: 7.524717553229688e-08\n",
      "Trained model with layers: 9, itertation: 19, fcnot_loss: 2.7877519926234643e-08\n",
      "Trained model with layers: 9, itertation: 20, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 9.5, itertation: 1, fcnot_loss: 3.650024149988857e-08\n",
      "Trained model with layers: 9.5, itertation: 2, fcnot_loss: 5.268356063861754e-08\n",
      "Trained model with layers: 9.5, itertation: 3, fcnot_loss: 6.989264130329236e-08\n",
      "Trained model with layers: 9.5, itertation: 4, fcnot_loss: 6.322027276634104e-08\n",
      "Trained model with layers: 9.5, itertation: 5, fcnot_loss: 3.942476676500724e-08\n",
      "Trained model with layers: 9.5, itertation: 6, fcnot_loss: 1.2377834115800373e-07\n",
      "Trained model with layers: 9.5, itertation: 7, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 9.5, itertation: 8, fcnot_loss: 3.332000937312528e-08\n",
      "Trained model with layers: 9.5, itertation: 9, fcnot_loss: 8.161702118890973e-08\n",
      "Trained model with layers: 9.5, itertation: 10, fcnot_loss: 5.268356063861754e-08\n",
      "Trained model with layers: 9.5, itertation: 11, fcnot_loss: 1.1445799094242763e-07\n",
      "Trained model with layers: 9.5, itertation: 12, fcnot_loss: 1.320246245463889e-07\n",
      "Trained model with layers: 9.5, itertation: 13, fcnot_loss: 6.143906154658885e-08\n",
      "Trained model with layers: 9.5, itertation: 14, fcnot_loss: 6.05288029062447e-08\n",
      "Trained model with layers: 9.5, itertation: 15, fcnot_loss: 4.9421560620597e-08\n",
      "Trained model with layers: 9.5, itertation: 16, fcnot_loss: 5.7711949142924205e-08\n",
      "Trained model with layers: 9.5, itertation: 17, fcnot_loss: 3.7990655851310376e-08\n",
      "Trained model with layers: 9.5, itertation: 18, fcnot_loss: 9.483040914951157e-08\n",
      "Trained model with layers: 9.5, itertation: 19, fcnot_loss: 1.0745380149674383e-07\n",
      "Trained model with layers: 9.5, itertation: 20, fcnot_loss: 7.300048299977713e-08\n",
      "Trained model with layers: 10, itertation: 1, fcnot_loss: 1.311810080830679e-07\n",
      "Trained model with layers: 10, itertation: 2, fcnot_loss: 7.670842216154189e-08\n",
      "Trained model with layers: 10, itertation: 3, fcnot_loss: 7.068241373080863e-08\n",
      "Trained model with layers: 10, itertation: 4, fcnot_loss: 4.0808510594454864e-08\n",
      "Trained model with layers: 10, itertation: 5, fcnot_loss: 6.746787680787904e-08\n",
      "Trained model with layers: 10, itertation: 6, fcnot_loss: 8.815645848363066e-08\n",
      "Trained model with layers: 10, itertation: 7, fcnot_loss: 8.024520884654677e-08\n",
      "Trained model with layers: 10, itertation: 8, fcnot_loss: 3.161013638317052e-08\n",
      "Trained model with layers: 10, itertation: 9, fcnot_loss: 8.560065398421926e-08\n",
      "Trained model with layers: 10, itertation: 10, fcnot_loss: 9.18569267238524e-08\n",
      "Trained model with layers: 10, itertation: 11, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 10, itertation: 12, fcnot_loss: 6.828569911354542e-08\n",
      "Trained model with layers: 10, itertation: 13, fcnot_loss: 4.828528090125218e-08\n",
      "Trained model with layers: 10, itertation: 14, fcnot_loss: 5.1619136559035694e-08\n",
      "Trained model with layers: 10, itertation: 15, fcnot_loss: 3.942476676500724e-08\n",
      "Trained model with layers: 10, itertation: 16, fcnot_loss: 4.9421560620597e-08\n",
      "Trained model with layers: 10, itertation: 17, fcnot_loss: 1.06935948836858e-07\n",
      "Trained model with layers: 10, itertation: 18, fcnot_loss: 9.06402249678704e-08\n",
      "Trained model with layers: 10, itertation: 19, fcnot_loss: 6.322027276634104e-08\n",
      "Trained model with layers: 10, itertation: 20, fcnot_loss: 1.4901161193847656e-08\n",
      "Trained model with layers: 10.5, itertation: 1, fcnot_loss: 6.989264130329236e-08\n",
      "Trained model with layers: 10.5, itertation: 2, fcnot_loss: 4.828528090125218e-08\n",
      "Trained model with layers: 10.5, itertation: 3, fcnot_loss: 7.598131170262075e-08\n",
      "Trained model with layers: 10.5, itertation: 4, fcnot_loss: 5.1619136559035694e-08\n",
      "Trained model with layers: 10.5, itertation: 5, fcnot_loss: 8.093402155637902e-08\n",
      "Trained model with layers: 10.5, itertation: 6, fcnot_loss: 6.143906154658885e-08\n",
      "Trained model with layers: 10.5, itertation: 7, fcnot_loss: 5.475036224983286e-08\n",
      "Trained model with layers: 10.5, itertation: 8, fcnot_loss: 5.7711949142924205e-08\n",
      "Trained model with layers: 10.5, itertation: 9, fcnot_loss: 5.5755039852469285e-08\n",
      "Trained model with layers: 10.5, itertation: 10, fcnot_loss: 6.989264130329236e-08\n",
      "Trained model with layers: 10.5, itertation: 11, fcnot_loss: 5.053229617420784e-08\n",
      "Trained model with layers: 10.5, itertation: 12, fcnot_loss: 7.81423485405856e-08\n",
      "Trained model with layers: 10.5, itertation: 13, fcnot_loss: 4.470348358154297e-08\n",
      "Trained model with layers: 10.5, itertation: 14, fcnot_loss: 5.475036224983286e-08\n",
      "Trained model with layers: 10.5, itertation: 15, fcnot_loss: 1.320246245463889e-07\n",
      "Trained model with layers: 10.5, itertation: 16, fcnot_loss: 6.989264130329236e-08\n",
      "Trained model with layers: 10.5, itertation: 17, fcnot_loss: 2.5809568279517847e-08\n",
      "Trained model with layers: 10.5, itertation: 18, fcnot_loss: 9.30577218293905e-08\n",
      "Trained model with layers: 10.5, itertation: 19, fcnot_loss: 6.989264130329236e-08\n",
      "Trained model with layers: 10.5, itertation: 20, fcnot_loss: 6.143906154658885e-08\n",
      "Trained model with layers: 11, itertation: 1, fcnot_loss: 6.828569911354542e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmolina/Documents/GitFolder/EE514-final-project/basic_neural_networks/utils.py:91: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(1 - (1/4)*abs(loss))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m np\u001b[38;5;241m.\u001b[39msave(path, m)\n\u001b[0;32m---> 30\u001b[0m magnitude \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor(\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_loss\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m magnitudes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(magnitude)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained model with layers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, itertation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, fcnot_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def make_dir_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "\n",
    "layers_arr = []\n",
    "f_loss_arr = []\n",
    "params_arr = []\n",
    "magnitudes = []\n",
    "for layers in range(3, 12, 1):\n",
    "    for addHalf in [False, True]:\n",
    "        l = layers + 0.5 if addHalf else layers\n",
    "        currentPath = os.getcwd()\n",
    "        directory = os.path.join(currentPath, f'trained_models/layers_{l}')\n",
    "        make_dir_if_not_exists(directory)\n",
    "        layers_arr.append([])\n",
    "        f_loss_arr.append([])\n",
    "        params_arr.append([])\n",
    "        magnitudes.append([])\n",
    "        for i in range(1,21,1):\n",
    "            layers_arr[-1].append(l)\n",
    "            m, p, f_loss = get_optimal_half_matrix(num_layers=layers, addHalf=addHalf)\n",
    "            f_loss_arr[-1].append(f_loss)\n",
    "            params_arr[-1].append(p)\n",
    "            path = os.path.join(directory, f'model_{i}.txt')\n",
    "            np.save(path, m)\n",
    "            magnitude = math.floor(math.log10(f_loss))\n",
    "            magnitudes[-1].append(magnitude)\n",
    "            print(f\"Trained model with layers: {l}, itertation: {i}, fcnot_loss: {f_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model with layers: 11, itertation: 1, fcnot_loss: 1.0848209096909341e-07\n",
      "Trained model with layers: 11, itertation: 2, fcnot_loss: 7.670842216154189e-08\n",
      "Trained model with layers: 11, itertation: 3, fcnot_loss: 6.495265578539184e-08\n",
      "Trained model with layers: 11, itertation: 4, fcnot_loss: 9.424321830774485e-08\n",
      "Trained model with layers: 11, itertation: 5, fcnot_loss: 7.742870483855355e-08\n",
      "Trained model with layers: 11, itertation: 6, fcnot_loss: 9.599401598218922e-08\n",
      "Trained model with layers: 11, itertation: 7, fcnot_loss: 4.712160915387242e-08\n",
      "Trained model with layers: 11, itertation: 8, fcnot_loss: 7.300048299977713e-08\n",
      "Trained model with layers: 11, itertation: 9, fcnot_loss: 6.314998895054189e-07\n",
      "Trained model with layers: 11, itertation: 10, fcnot_loss: 2.356080457693621e-08\n",
      "Trained model with layers: 11, itertation: 11, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 11, itertation: 12, fcnot_loss: 4.9421560620597e-08\n",
      "Trained model with layers: 11, itertation: 13, fcnot_loss: 7.598131170262075e-08\n",
      "Trained model with layers: 11, itertation: 14, fcnot_loss: 3.7990655851310376e-08\n",
      "Trained model with layers: 11, itertation: 15, fcnot_loss: 1.1299364930468128e-07\n",
      "Trained model with layers: 11, itertation: 16, fcnot_loss: 3.942476676500724e-08\n",
      "Trained model with layers: 11, itertation: 17, fcnot_loss: 8.494968899333014e-08\n",
      "Trained model with layers: 11, itertation: 18, fcnot_loss: 6.05288029062447e-08\n",
      "Trained model with layers: 11, itertation: 19, fcnot_loss: 5.7711949142924205e-08\n",
      "Trained model with layers: 11, itertation: 20, fcnot_loss: 2.356080457693621e-08\n",
      "Trained model with layers: 11.5, itertation: 1, fcnot_loss: 6.233602959916559e-08\n",
      "Trained model with layers: 11.5, itertation: 2, fcnot_loss: 8.688795409866125e-08\n",
      "Trained model with layers: 11.5, itertation: 3, fcnot_loss: 5.5755039852469285e-08\n",
      "Trained model with layers: 11.5, itertation: 4, fcnot_loss: 4.0808510594454864e-08\n",
      "Trained model with layers: 11.5, itertation: 5, fcnot_loss: 5.1619136559035694e-08\n",
      "Trained model with layers: 11.5, itertation: 6, fcnot_loss: 5.1619136559035694e-08\n",
      "Trained model with layers: 11.5, itertation: 7, fcnot_loss: 6.233602959916559e-08\n",
      "Trained model with layers: 11.5, itertation: 8, fcnot_loss: 6.989264130329236e-08\n",
      "Trained model with layers: 11.5, itertation: 9, fcnot_loss: 5.7711949142924205e-08\n",
      "Trained model with layers: 11.5, itertation: 10, fcnot_loss: 4.2146848510894035e-08\n",
      "Trained model with layers: 11.5, itertation: 11, fcnot_loss: 8.429369702178807e-08\n",
      "Trained model with layers: 11.5, itertation: 12, fcnot_loss: 6.495265578539184e-08\n",
      "Trained model with layers: 11.5, itertation: 13, fcnot_loss: 5.7711949142924205e-08\n",
      "Trained model with layers: 11.5, itertation: 14, fcnot_loss: 5.3726900748371916e-08\n",
      "Trained model with layers: 11.5, itertation: 15, fcnot_loss: 6.143906154658885e-08\n",
      "Trained model with layers: 11.5, itertation: 16, fcnot_loss: 5.960464477539063e-08\n",
      "Trained model with layers: 11.5, itertation: 17, fcnot_loss: 5.5755039852469285e-08\n",
      "Trained model with layers: 11.5, itertation: 18, fcnot_loss: 4.470348358154297e-08\n",
      "Trained model with layers: 11.5, itertation: 19, fcnot_loss: 1.2377834115800373e-07\n",
      "Trained model with layers: 11.5, itertation: 20, fcnot_loss: 5.674193133312396e-08\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "layers = 11\n",
    "for addHalf in [False, True]:\n",
    "    l = layers + 0.5 if addHalf else layers\n",
    "    currentPath = os.getcwd()\n",
    "    directory = os.path.join(currentPath, f'trained_models/layers_{l}')\n",
    "    make_dir_if_not_exists(directory)\n",
    "    layers_arr.append([])\n",
    "    f_loss_arr.append([])\n",
    "    params_arr.append([])\n",
    "    magnitudes.append([])\n",
    "    for i in range(1,21,1):\n",
    "        layers_arr[-1].append(l)\n",
    "        m, p, f_loss = get_optimal_half_matrix(num_layers=layers, addHalf=addHalf)\n",
    "        f_loss_arr[-1].append(f_loss)\n",
    "        params_arr[-1].append(p)\n",
    "        path = os.path.join(directory, f'model_{i}.txt')\n",
    "        np.save(path, m)\n",
    "        magnitude = math.floor(math.log10(f_loss)) if f_loss != 0 else -100\n",
    "        magnitudes[-1].append(magnitude)\n",
    "        print(f\"Trained model with layers: {l}, itertation: {i}, fcnot_loss: {f_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "len(layers_arr), len(f_loss_arr), len(params_arr), len(magnitudes)\n",
    "\n",
    "# pulses vs loss\n",
    "# layers vs loss\n",
    "# time steps vs loss\n",
    "average_loss = []\n",
    "layers = [layers_arr[i][0] for i in range(len(layers_arr))]\n",
    "def get_number_of_pulses(layer):\n",
    "    if int(math.floor(layer)) == layer:\n",
    "        return layer * 5\n",
    "    else:\n",
    "        return (layer-0.5) * 5 + 3\n",
    "def get_number_of_time_steps(layer):\n",
    "    if int(math.floor(layer)) == layer:\n",
    "        return layer * 2\n",
    "    else:\n",
    "        return (layer-0.5) * 2 + 1\n",
    "pulses = [get_number_of_pulses(l) for l in layers]\n",
    "time_steps = [get_number_of_time_steps(l) for l in layers]\n",
    "for i in range(len(layers_arr)):\n",
    "    # powers = [math.ceil(math.log10(f_loss)) for f_loss in f_loss_arr[i]]\n",
    "    avg_loss = np.mean(f_loss_arr[i])\n",
    "    magnitude = round(math.log10(avg_loss))\n",
    "    average_loss.append(magnitude)\n",
    "    # std_loss = np.std(f_loss_arr[i])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'layers': layers,\n",
    "    'pulses': pulses,\n",
    "    'time_steps': time_steps,\n",
    "    'loss': average_loss\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, sharey=False, figsize=(15,5))\n",
    "\n",
    "\n",
    "ax[0].plot(df['layers'], df['loss'], 'o-')\n",
    "ax[0].set_xlabel('Layers')\n",
    "ax[0].set_xticks(range(3, 12, 1))\n",
    "ax[0].set_ylabel('Loss Magnitude 10^y')\n",
    "ax[0].set_title('Layers vs Loss')\n",
    "\n",
    "ax[1].plot(df['pulses'], df['loss'], '*-')\n",
    "ax[1].set_xlabel('Pulses')\n",
    "ax[1].set_ylabel('Loss Magnitude 10^y')\n",
    "ax[1].set_title('Pulses vs Loss')\n",
    "\n",
    "ax[2].plot(df['time_steps'], df['loss'], 'x-')\n",
    "ax[2].set_xlabel('Time Steps')\n",
    "ax[2].set_ylabel('Loss Magnitude 10^y')\n",
    "ax[2].set_title('Time Steps vs Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.332000937312528e-08 -8\n"
     ]
    }
   ],
   "source": [
    "from utils import MatrixUtils\n",
    "import math\n",
    "\n",
    "mu = MatrixUtils()\n",
    "predStates = [np.matmul(m1, mat) for mat in inputStates]\n",
    "to = mu.f_cnot_loss(expectedStates, predStates)\n",
    "\n",
    "print(to,po)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing a minimization function suggested by ChatPGT and modified for our specific problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_matrix(num_layers, half=False):\n",
    "    import numpy as np\n",
    "    from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "    from utils import MatrixUtils\n",
    "    matrixUtils = MatrixUtils()\n",
    "    # Define the correct operations you want the matrix to perform on basis vectors\n",
    "    def target_operations(parameters, inputStates):\n",
    "        # Reshape the parameters into the matrix form\n",
    "        parameters = np.reshape(parameters, (num_layers+1 if half else num_layers, 5))\n",
    "        matrix = matrixUtils.get_total_matrix(size_of_vec=2**6, weights=parameters)\n",
    "\n",
    "        # Perform matrix multiplication with basis vectors\n",
    "        results = []\n",
    "        for i in range(len(inputStates)):\n",
    "            results.append(np.matmul(matrix, inputStates[i]))\n",
    "\n",
    "        # Define the target operations you want (modify this based on your specific task)\n",
    "        target_result = np.array(expectedStates)\n",
    "\n",
    "        # Calculate the loss as the difference between the obtained result and the target result\n",
    "        # loss = square_loss(target_result, results)\n",
    "        loss = matrixUtils.f_cnot_loss(target_result, results)\n",
    "        return loss\n",
    "\n",
    "    # Generate random basis vectors and target result\n",
    "    basis_vectors = np.array(inputStates)\n",
    "    target_result = np.array(expectedStates)\n",
    "\n",
    "    # Flatten the matrix parameters for optimization\n",
    "    initial_parameters = np.ndarray.flatten(matrixUtils.get_random_weights(num_layers))\n",
    "\n",
    "    # Use scipy's minimize function to optimize the parameters\n",
    "    result = minimize(target_operations, initial_parameters, args=(basis_vectors,), method='L-BFGS-B')\n",
    "\n",
    "    # Reshape the optimized parameters back into the matrix form\n",
    "    optimized_matrix = matrixUtils.get_total_matrix(size_of_vec=2**6, weights=result.x.reshape((num_layers, 5)))\n",
    "\n",
    "    # print(\"Optimized Matrix:\")\n",
    "    # print(optimized_matrix)\n",
    "\n",
    "    predStates = [np.matmul(optimized_matrix, mat) for mat in inputStates]\n",
    "    print(f\"f_cnot_loss for {num_layers} layers = {matrixUtils.f_cnot_loss(expectedStates, predStates)}\")\n",
    "    print(f\"square_loss for {num_layers} layers = {matrixUtils.square_loss(expectedStates, predStates)}\")\n",
    "    return optimized_matrix, result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_cnot_loss for 8 layers = 1.4675941767984784e-07\n",
      "square_loss for 8 layers = 2.9009743491920505e-28\n",
      "f_cnot_loss for 12 layers = 2.1073424255447017e-08\n",
      "square_loss for 12 layers = 1.6640034719505718e-31\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "matrix1, params1 = get_optimal_matrix(num_layers=8)\n",
    "matrix2, params2 = get_optimal_matrix(num_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params1=[-0.53776279  0.01087519 -0.97064673 -0.41250992  0.06527786 -0.6740068\n",
      "  0.46309053  0.46493861  0.86789249  0.24952521 -0.00817814 -0.58525397\n",
      " -0.49847904  0.26401364  0.58901837  0.47484948 -0.51859519  0.48456201\n",
      "  0.96868777 -0.75722557 -0.35263081  1.12137384  0.01643639 -0.26469552\n",
      " -0.51133436  0.02086598 -0.5129765  -0.91082416 -0.72381358  0.70035167\n",
      " -0.83228161  0.30341447  0.20024559 -0.12199987  0.69083451 -0.76650945\n",
      "  0.10490144  0.33381965  0.03346551  0.46261457]\n",
      "params2=[-0.89640595  0.44511122 -0.07451297  0.22047689  1.05412178 -0.13885729\n",
      "  0.62525218  0.74385162  0.76362229  0.84292     0.40638121  0.70929019\n",
      "  0.60906231 -0.92276944  0.61712147  0.38767971  0.70527334 -0.5893029\n",
      "  0.59438139  0.89513418  0.16828765  0.82446941  0.90477795  0.13986333\n",
      " -0.42391503 -0.50445723 -0.67671051 -0.35620053  0.42677777 -0.78969741\n",
      " -0.91291488  0.23733852  0.17391615 -0.46338381 -0.14638486  0.91769235\n",
      "  0.05951    -0.09475124  0.62889173 -0.40302423 -1.0413838  -0.3876081\n",
      "  0.24429313 -0.02375021  0.6172341   0.32383211 -0.68770129 -0.11628581\n",
      " -0.30774243 -0.48427985  0.46276477 -0.30152962  0.06596537 -0.63814308\n",
      "  0.86789804  0.93640488 -0.76730465 -0.57257597 -0.21101739  0.42075565]\n",
      "MSE=0.03509326260655889\n"
     ]
    }
   ],
   "source": [
    "flat = lambda mat : np.ndarray.flatten(mat)\n",
    "amp_sqrd = lambda a : np.conjugate(a)*a\n",
    "\n",
    "print(f'params1={params1}')\n",
    "print(f'params2={params2}')\n",
    "\n",
    "temp = np.matmul(matrix1, np.conjugate(np.transpose(matrix2)))\n",
    "target = np.eye(temp.shape[0])\n",
    "\n",
    "\n",
    "def mean_squared_error_complex(a1,a2):\n",
    "    n = len(a1)\n",
    "    t = 0\n",
    "    for i in range(n):\n",
    "        t += amp_sqrd(a1[i] - a2[i])\n",
    "    return np.real(t/n)\n",
    "\n",
    "mse = mean_squared_error_complex(flat(temp), flat(target))\n",
    "\n",
    "print(f'MSE={mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempted to make a Neural Network model but wasn't working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(35,) dtype=float32, numpy=\narray([ 2.5269053 ,  0.5212329 ,  0.9739547 ,  2.5969772 ,  0.6503308 ,\n        2.1297295 , -1.931953  ,  0.7041187 , -1.3276408 , -2.2411902 ,\n        1.2720861 ,  1.5847887 ,  3.0150158 ,  1.2192807 , -1.7325749 ,\n       -0.82495993, -1.756478  , -1.1077476 , -2.713036  ,  1.8335059 ,\n        1.614816  ,  2.2740006 ,  2.2506728 , -1.1599189 , -2.0895307 ,\n        3.0720863 , -0.31033638, -0.30215982, -2.318698  , -3.134845  ,\n       -1.3226986 , -2.8566093 ,  1.5320274 ,  0.39243674,  0.13296835],\n      dtype=float32)>),).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(target_operations(parameters, inputVectors))\n\u001b[1;32m     50\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, [parameters])\n\u001b[0;32m---> 51\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1222\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1218\u001b[0m experimental_aggregate_gradients \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperimental_aggregate_gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m )\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m-> 1222\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1184\u001b[0m, in \u001b[0;36mOptimizer.aggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grads_and_vars\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce_sum_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/utils.py:33\u001b[0m, in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all-reduced gradients aggregated via summation.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m  List of (gradient, variable) pairs where gradients have been all-reduced.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(grads_and_vars)\n\u001b[0;32m---> 33\u001b[0m filtered_grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_grads_and_vars:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mstrategy_supports_no_merge_call():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[1;32m     76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     82\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m when minimizing the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss. If you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using `model.compile()`, did you forget to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide a `loss` argument?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vars_with_empty_grads]),\n\u001b[1;32m     87\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(35,) dtype=float32, numpy=\narray([ 2.5269053 ,  0.5212329 ,  0.9739547 ,  2.5969772 ,  0.6503308 ,\n        2.1297295 , -1.931953  ,  0.7041187 , -1.3276408 , -2.2411902 ,\n        1.2720861 ,  1.5847887 ,  3.0150158 ,  1.2192807 , -1.7325749 ,\n       -0.82495993, -1.756478  , -1.1077476 , -2.713036  ,  1.8335059 ,\n        1.614816  ,  2.2740006 ,  2.2506728 , -1.1599189 , -2.0895307 ,\n        3.0720863 , -0.31033638, -0.30215982, -2.318698  , -3.134845  ,\n       -1.3226986 , -2.8566093 ,  1.5320274 ,  0.39243674,  0.13296835],\n      dtype=float32)>),)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def f_cnot_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(np.size(y_true)//(2**n_qubits)):\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += fidelity\n",
    "    return np.sqrt(1 - (1/4)*abs(loss))\n",
    "\n",
    "from utils import MatrixUtils\n",
    "matrixUtils = MatrixUtils()\n",
    "\n",
    "# Define the correct operations you want the matrix to perform on basis vectors\n",
    "def target_operations(parameters, inputVectors):\n",
    "    # Perform matrix multiplication with basis vectors\n",
    "    # Reshape the parameters into the matrix form\n",
    "    params = np.reshape(parameters, (num_layers, 5))\n",
    "    matrix = tf.constant(matrixUtils.get_total_matrix(2**6, params), dtype=tf.complex128)\n",
    "    # results = tf.matmul(matrix, inputVectors, transpose_b=True)\n",
    "    results = []\n",
    "    for i in range(np.size(inputVectors)//(2**n_qubits)):\n",
    "        results.append(np.matmul(matrix, inputVectors[i]))\n",
    "    results = tf.constant(results, dtype=tf.complex128)\n",
    "\n",
    "    # Define the target operations you want (modify this based on your specific task)\n",
    "    target_results = tf.constant(expectedStates, dtype=tf.complex128)\n",
    "\n",
    "    # Calculate the loss as the mean squared error between the obtained result and the target result\n",
    "    loss = f_cnot_loss(target_results, results)\n",
    "    return loss\n",
    "\n",
    "# Example: Set the number of basis vectors and their dimensionality\n",
    "num_vectors = 4\n",
    "vector_dimension = 2**n_qubits\n",
    "\n",
    "# Generate random basis vectors and target result\n",
    "inputVectors = tf.constant(inputStates, dtype=tf.complex128)\n",
    "\n",
    "# Define the matrix as a TensorFlow Variable\n",
    "parameters = tf.Variable(np.ndarray.flatten(matrixUtils.get_random_weights(num_layers)), dtype=tf.float32)\n",
    "\n",
    "# Use an optimizer to minimize the loss\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.constant(target_operations(parameters, inputVectors))\n",
    "\n",
    "    gradients = tape.gradient(loss, [parameters])\n",
    "    optimizer.apply_gradients(zip(gradients, [parameters]))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Get the optimized matrix\n",
    "parameters = np.reshape(parameters, (num_layers, 5))\n",
    "matrix = matrixUtils.get_total_matrix(2**6,parameters)\n",
    "optimized_matrix = matrix.numpy()\n",
    "\n",
    "print(\"Optimized Matrix:\")\n",
    "print(optimized_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual matrix multiplication of what the paper suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_cnot loss = 0.7189360747413388\n"
     ]
    }
   ],
   "source": [
    "p1 = np.arccos(-1/np.sqrt(3))/np.pi\n",
    "p2 = np.arcsin(1/3)/np.pi\n",
    "\n",
    "\n",
    "def Id_n(n):\n",
    "    assert n >= 0\n",
    "    if n==0:\n",
    "        return 1\n",
    "    temp = I\n",
    "    for i in range(n-1):\n",
    "        temp = np.kron(temp, I)\n",
    "    return temp\n",
    "\n",
    "matrixUtils = MatrixUtils()\n",
    "U_ex = lambda p : matrixUtils.U_ex(p, scale=np.pi)\n",
    "\n",
    "\n",
    "bounds = [[3,4],[2,5],[3,4],[2,5],[1,4],[2,5],[1,4],[2,5],[1,4],[2,5],[3,4],[2,5],[3,4]]\n",
    "operators = [matrixUtils.U_ex(p1, scale=np.pi),\n",
    "np.kron(U_ex(1/2),U_ex(p2)),\n",
    "U_ex(1),\n",
    "np.kron(U_ex(-1/2),U_ex(-1/2)),\n",
    "np.kron(U_ex(1),U_ex(-1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(1)),\n",
    "np.kron(U_ex(-1/2),U_ex(1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(1)),\n",
    "np.kron(U_ex(1),U_ex(-1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(-1/2)),\n",
    "U_ex(1),\n",
    "np.kron(U_ex(1/2),U_ex(1-p2)),\n",
    "U_ex(-p1)]\n",
    "\n",
    "\n",
    "newOps = []\n",
    "for i, (start, end) in enumerate(bounds):\n",
    "    temp = matrixUtils.nestedKron(Id_n(start),operators[i], Id_n(5-end))\n",
    "    newOps.append(temp.copy())\n",
    "\n",
    "totalOperator = np.eye(2**6)\n",
    "for op in newOps:\n",
    "    totalOperator = np.matmul(op,totalOperator)\n",
    "\n",
    "U_cnot = totalOperator.copy()\n",
    "\n",
    "def f_cnot_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(np.size(y_true)//(2**n_qubits)):\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += fidelity\n",
    "    return np.sqrt(1 - (1/4)*abs(loss))\n",
    "\n",
    "\n",
    "predStates = [np.matmul(U_cnot, mat) for mat in inputStates]\n",
    "print(f\"f_cnot loss = {f_cnot_loss(expectedStates, predStates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
