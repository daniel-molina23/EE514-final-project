{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the trianable ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pennylane as qml\n",
    "\n",
    "I = np.eye(2)\n",
    "n_qubits = 6\n",
    "size_of_vec = 2**n_qubits\n",
    "num_layers = 5\n",
    "\n",
    "from utils import DFS\n",
    "inputStates, expectedStates = DFS().getInitialTargetStates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing a minimization function suggested by ChatPGT and modified for our specific problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Matrix:\n",
      "[[-0.27353149-0.96186305j  0.        +0.j          0.        +0.j\n",
      "  ...  0.        +0.j          0.        +0.j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j         -0.30504112-0.00295081j  0.29018547+0.27333573j\n",
      "  ...  0.        +0.j          0.        +0.j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.36993817-0.06883858j -0.02443   +0.11591914j\n",
      "  ...  0.        +0.j          0.        +0.j\n",
      "   0.        +0.j        ]\n",
      " ...\n",
      " [ 0.        +0.j          0.        +0.j          0.        +0.j\n",
      "  ... -0.02443   +0.11591914j  0.36993817-0.06883858j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.        +0.j          0.        +0.j\n",
      "  ...  0.29018547+0.27333573j -0.30504112-0.00295081j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.        +0.j          0.        +0.j\n",
      "  ...  0.        +0.j          0.        +0.j\n",
      "  -0.27353149-0.96186305j]]\n",
      "f_cnot_loss = 2.3654860059004343e-07\n",
      "square_loss = 1.9445606202972602e-27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pennylane as qml\n",
    "\n",
    "\n",
    "Udot = lambda s1, U, s2 : np.dot(np.conjugate(np.transpose(s1)),np.matmul(U,s2))\n",
    "\n",
    "def nestedKron(*args): # use \"*args\" to access an array of inputs\n",
    "    assert len(args) >= 2\n",
    "    temp = args[0]\n",
    "    for arg in args[1:]:\n",
    "        temp = np.kron(temp, arg)\n",
    "    return temp\n",
    "\n",
    "def get_random_weights(num_layers):\n",
    "    return 2 * np.pi * np.random.random(size=(num_layers, 5)) - np.pi\n",
    "\n",
    "def U_ex(p):\n",
    "    from scipy.linalg import expm\n",
    "    X = [[0,1],[1,0]]\n",
    "    Y = np.array([[0,-1j],[1j,0]], dtype=np.complex128)\n",
    "    Z = [[1,0],[0,-1]]\n",
    "\n",
    "    H_ex = (1/4)*(np.kron(X,X) + np.kron(Y,Y) + np.kron(Z,Z))\n",
    "    # print(f'H_ex.type = {type(H_ex)}')\n",
    "    U_exchange = expm(-1j*p*H_ex) # p is now -pi to pi\n",
    "    return np.array(U_exchange)\n",
    "\n",
    "def single_layer_U(layer_weights):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    firstPart = nestedKron(U_ex(layer_weights[0]), U_ex(layer_weights[1]), U_ex(layer_weights[2]))\n",
    "    secondPart = nestedKron(I, U_ex(layer_weights[3]), U_ex(layer_weights[4]), I)\n",
    "    return np.matmul(secondPart, firstPart)\n",
    "\n",
    "def get_matrix(weights):\n",
    "    totalMatrix = np.eye(size_of_vec)\n",
    "    for layer_weights in weights:\n",
    "        mat = single_layer_U(layer_weights)\n",
    "        totalMatrix = np.matmul(totalMatrix, mat)\n",
    "    return totalMatrix\n",
    "\n",
    "def square_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(expectedStates)):\n",
    "        # c = np.dot(np.conjugate(expectedStates[i]), predictedStates[i])\n",
    "        # c_2 = self.amp_sqrd(c)\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += (1 - fidelity) ** 2\n",
    "    loss /= len(expectedStates)\n",
    "    return 0.5*loss\n",
    "\n",
    "def f_cnot_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(expectedStates)):\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += fidelity\n",
    "    return np.sqrt(1 - (1/4)*abs(loss))\n",
    "\n",
    "# Define the correct operations you want the matrix to perform on basis vectors\n",
    "def target_operations(parameters, inputStates):\n",
    "    # Reshape the parameters into the matrix form\n",
    "    parameters = np.reshape(parameters, (num_layers, 5))\n",
    "    matrix = get_matrix(parameters)\n",
    "\n",
    "    # Perform matrix multiplication with basis vectors\n",
    "    results = []\n",
    "    for i in range(len(inputStates)):\n",
    "        results.append(np.matmul(matrix, inputStates[i]))\n",
    "\n",
    "    # Define the target operations you want (modify this based on your specific task)\n",
    "    target_result = np.array(expectedStates)\n",
    "\n",
    "    # Calculate the loss as the difference between the obtained result and the target result\n",
    "    # loss = square_loss(target_result, results)\n",
    "    loss = f_cnot_loss(target_result, results)\n",
    "    return loss\n",
    "\n",
    "# Example: Set the number of basis vectors and their dimensionality\n",
    "num_vectors = 4\n",
    "vector_dimension = size_of_vec\n",
    "\n",
    "# Generate random basis vectors and target result\n",
    "basis_vectors = np.array(inputStates)\n",
    "target_result = np.array(expectedStates)\n",
    "\n",
    "# Flatten the matrix parameters for optimization\n",
    "initial_parameters = np.ndarray.flatten(get_random_weights(num_layers))\n",
    "\n",
    "# Use scipy's minimize function to optimize the parameters\n",
    "result = minimize(target_operations, initial_parameters, args=(basis_vectors,), method='L-BFGS-B')\n",
    "\n",
    "# Reshape the optimized parameters back into the matrix form\n",
    "optimized_matrix = get_matrix(result.x.reshape((num_layers, 5)))\n",
    "\n",
    "print(\"Optimized Matrix:\")\n",
    "print(optimized_matrix)\n",
    "\n",
    "predStates = [np.matmul(optimized_matrix, mat) for mat in inputStates]\n",
    "print(f\"f_cnot_loss = {f_cnot_loss(expectedStates, predStates)}\")\n",
    "print(f\"square_loss = {square_loss(expectedStates, predStates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempted to make a Neural Network model but wasn't working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(35,) dtype=float32, numpy=\narray([ 2.5269053 ,  0.5212329 ,  0.9739547 ,  2.5969772 ,  0.6503308 ,\n        2.1297295 , -1.931953  ,  0.7041187 , -1.3276408 , -2.2411902 ,\n        1.2720861 ,  1.5847887 ,  3.0150158 ,  1.2192807 , -1.7325749 ,\n       -0.82495993, -1.756478  , -1.1077476 , -2.713036  ,  1.8335059 ,\n        1.614816  ,  2.2740006 ,  2.2506728 , -1.1599189 , -2.0895307 ,\n        3.0720863 , -0.31033638, -0.30215982, -2.318698  , -3.134845  ,\n       -1.3226986 , -2.8566093 ,  1.5320274 ,  0.39243674,  0.13296835],\n      dtype=float32)>),).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(target_operations(parameters, inputVectors))\n\u001b[1;32m     50\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, [parameters])\n\u001b[0;32m---> 51\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1222\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1218\u001b[0m experimental_aggregate_gradients \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperimental_aggregate_gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m )\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m-> 1222\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1184\u001b[0m, in \u001b[0;36mOptimizer.aggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grads_and_vars\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce_sum_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/utils.py:33\u001b[0m, in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all-reduced gradients aggregated via summation.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m  List of (gradient, variable) pairs where gradients have been all-reduced.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(grads_and_vars)\n\u001b[0;32m---> 33\u001b[0m filtered_grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_grads_and_vars:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mstrategy_supports_no_merge_call():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[1;32m     76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     82\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m when minimizing the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss. If you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using `model.compile()`, did you forget to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide a `loss` argument?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vars_with_empty_grads]),\n\u001b[1;32m     87\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(35,) dtype=float32, numpy=\narray([ 2.5269053 ,  0.5212329 ,  0.9739547 ,  2.5969772 ,  0.6503308 ,\n        2.1297295 , -1.931953  ,  0.7041187 , -1.3276408 , -2.2411902 ,\n        1.2720861 ,  1.5847887 ,  3.0150158 ,  1.2192807 , -1.7325749 ,\n       -0.82495993, -1.756478  , -1.1077476 , -2.713036  ,  1.8335059 ,\n        1.614816  ,  2.2740006 ,  2.2506728 , -1.1599189 , -2.0895307 ,\n        3.0720863 , -0.31033638, -0.30215982, -2.318698  , -3.134845  ,\n       -1.3226986 , -2.8566093 ,  1.5320274 ,  0.39243674,  0.13296835],\n      dtype=float32)>),)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def f_cnot_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(np.size(y_true)//(2**n_qubits)):\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += fidelity\n",
    "    return np.sqrt(1 - (1/4)*abs(loss))\n",
    "\n",
    "\n",
    "# Define the correct operations you want the matrix to perform on basis vectors\n",
    "def target_operations(parameters, inputVectors):\n",
    "    # Perform matrix multiplication with basis vectors\n",
    "    # Reshape the parameters into the matrix form\n",
    "    params = np.reshape(parameters, (num_layers, 5))\n",
    "    matrix = tf.constant(get_matrix(params), dtype=tf.complex128)\n",
    "    # results = tf.matmul(matrix, inputVectors, transpose_b=True)\n",
    "    results = []\n",
    "    for i in range(np.size(inputVectors)//(2**n_qubits)):\n",
    "        results.append(np.matmul(matrix, inputVectors[i]))\n",
    "    results = tf.constant(results, dtype=tf.complex128)\n",
    "\n",
    "    # Define the target operations you want (modify this based on your specific task)\n",
    "    target_results = tf.constant(expectedStates, dtype=tf.complex128)\n",
    "\n",
    "    # Calculate the loss as the mean squared error between the obtained result and the target result\n",
    "    loss = f_cnot_loss(target_results, results)\n",
    "    return loss\n",
    "\n",
    "# Example: Set the number of basis vectors and their dimensionality\n",
    "num_vectors = 4\n",
    "vector_dimension = 2**n_qubits\n",
    "\n",
    "# Generate random basis vectors and target result\n",
    "inputVectors = tf.constant(inputStates, dtype=tf.complex128)\n",
    "\n",
    "# Define the matrix as a TensorFlow Variable\n",
    "parameters = tf.Variable(np.ndarray.flatten(get_random_weights(num_layers)), dtype=tf.float32)\n",
    "\n",
    "# Use an optimizer to minimize the loss\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.constant(target_operations(parameters, inputVectors))\n",
    "\n",
    "    gradients = tape.gradient(loss, [parameters])\n",
    "    optimizer.apply_gradients(zip(gradients, [parameters]))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Get the optimized matrix\n",
    "parameters = np.reshape(parameters, (num_layers, 5))\n",
    "matrix = get_matrix(parameters)\n",
    "optimized_matrix = matrix.numpy()\n",
    "\n",
    "print(\"Optimized Matrix:\")\n",
    "print(optimized_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual matrix multiplication of what the paper suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_cnot loss = 0.7189360747413388\n"
     ]
    }
   ],
   "source": [
    "p1 = np.arccos(-1/np.sqrt(3))/np.pi\n",
    "p2 = np.arcsin(1/3)/np.pi\n",
    "\n",
    "\n",
    "def Id_n(n):\n",
    "    assert n >= 0\n",
    "    if n==0:\n",
    "        return 1\n",
    "    temp = I\n",
    "    for i in range(n-1):\n",
    "        temp = np.kron(temp, I)\n",
    "    return temp\n",
    "\n",
    "\n",
    "bounds = [[3,4],[2,5],[3,4],[2,5],[1,4],[2,5],[1,4],[2,5],[1,4],[2,5],[3,4],[2,5],[3,4]]\n",
    "operators = [U_ex(p1),\n",
    "np.kron(U_ex(1/2),U_ex(p2)),\n",
    "U_ex(1),\n",
    "np.kron(U_ex(-1/2),U_ex(-1/2)),\n",
    "np.kron(U_ex(1),U_ex(-1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(1)),\n",
    "np.kron(U_ex(-1/2),U_ex(1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(1)),\n",
    "np.kron(U_ex(1),U_ex(-1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(-1/2)),\n",
    "U_ex(1),\n",
    "np.kron(U_ex(1/2),U_ex(1-p2)),\n",
    "U_ex(-p1)]\n",
    "\n",
    "\n",
    "newOps = []\n",
    "for i, (start, end) in enumerate(bounds):\n",
    "    temp = nestedKron(Id_n(start),operators[i], Id_n(5-end))\n",
    "    newOps.append(temp.copy())\n",
    "\n",
    "totalOperator = np.eye(2**6)\n",
    "for op in newOps:\n",
    "    totalOperator = np.matmul(op,totalOperator)\n",
    "\n",
    "U_cnot = totalOperator.copy()\n",
    "\n",
    "def f_cnot_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(np.size(y_true)//(2**n_qubits)):\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += fidelity\n",
    "    return np.sqrt(1 - (1/4)*abs(loss))\n",
    "\n",
    "\n",
    "predStates = [np.matmul(U_cnot, mat) for mat in inputStates]\n",
    "print(f\"f_cnot loss = {f_cnot_loss(expectedStates, predStates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
