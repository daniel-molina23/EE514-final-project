{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the trianable ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nestedKronecker(args): # use \"*args\" to access an array of inputs\n",
    "    assert len(args) >= 2\n",
    "    temp = args[0]\n",
    "    for arg in args[1:]:\n",
    "        temp = np.kron(temp, arg)\n",
    "    return temp\n",
    "\n",
    "basis = {0: [1,0], 1: [0,1], '0': [1,0], '1': [0,1]}\n",
    "\n",
    "basisVector = lambda binstr : nestedKronecker([basis[x] for x in binstr])\n",
    "\n",
    "# common states\n",
    "zero, one = basis['0'], basis['1']\n",
    "tplus = basisVector('11')\n",
    "tminus = basisVector('00')\n",
    "tzero = (1/np.sqrt(2))*(basisVector('01') + basisVector('10'))\n",
    "singlet = np.sqrt(1/2)*(basisVector('01') - basisVector('10'))\n",
    "\n",
    "\n",
    "# ------------------------ FOR STATE 1 ------------------------\n",
    "\n",
    "state1 = np.kron(np.kron(singlet, singlet), singlet)\n",
    "\n",
    "# ------------------------ FOR STATE 2 ------------------------\n",
    "\n",
    "largePyramid = np.sqrt(1/3)*(np.kron(tplus,tminus)+np.kron(tminus,tplus)-np.kron(tzero,tzero))\n",
    "state2 = np.kron(singlet,largePyramid)\n",
    "\n",
    "# ------------------------ FOR STATE 3 ------------------------\n",
    "\n",
    "state3 = np.kron(largePyramid,singlet)\n",
    "\n",
    "# ------------------------ FOR STATE 4 ------------------------\n",
    "\n",
    "# for psi0 and psi1 we are combining j1=1 and j2=1/2 (this is combinind the first peak and trough)\n",
    "# J = 1/2, M = -1/2\n",
    "psi0 = np.sqrt(1/3)*np.kron(tzero, zero) - np.sqrt(2/3)*np.kron(tminus, one)\n",
    "# J = 1/2, M = +1/2\n",
    "psi1 = np.sqrt(2/3)*np.kron(tplus, zero) - np.sqrt(1/3)*np.kron(tzero, one)\n",
    "\n",
    "\n",
    "# for phiminus, phizero, phiplus, we are are combining j1=1/2 and j2=1/2\n",
    "# J = 1, M = -1\n",
    "phiminus = np.kron(psi0,zero)\n",
    "# J = 1, M = 0\n",
    "phizero = np.sqrt(1/2)*(np.kron(psi1,zero) + np.kron(psi0,one))\n",
    "# J = 1, M = +1\n",
    "phiplus = np.kron(psi1,one)\n",
    "\n",
    "# J=0,M=0 and j1=1,j2=1\n",
    "state4 = np.sqrt(1/3)*(np.kron(phiplus, tminus) - np.kron(phizero, tzero) + np.kron(phiminus, tplus))\n",
    "\n",
    "# ------------------------ FOR STATE 5 ------------------------\n",
    "\n",
    "eta_minus3 = np.kron(tminus, basis['0'])\n",
    "eta_minus1 = np.sqrt(2/3)*np.kron(tzero,zero) + np.sqrt(1/3)*np.kron(tminus,one)\n",
    "eta_plus1 = np.sqrt(1/3)*np.kron(tplus,zero) + np.sqrt(2/3)*np.kron(tzero, one)\n",
    "eta_plus3 = np.kron(tplus,one)\n",
    "\n",
    "gamma_minus = np.sqrt(1/4)*np.kron(eta_minus1, zero) - np.sqrt(3/4)*np.kron(eta_minus3, one)\n",
    "gamma_zero = np.sqrt(1/2)*np.kron(eta_plus1, zero) - np.sqrt(1/2)*np.kron(eta_minus1,one)\n",
    "gamma_plus = np.sqrt(3/4)*np.kron(eta_plus3, zero) - np.sqrt(1/4)*np.kron(eta_plus1, one)\n",
    "\n",
    "state5 = np.sqrt(1/3)*(np.kron(gamma_plus,tminus) - np.kron(gamma_zero, tzero) - np.kron(gamma_minus, tplus))\n",
    "\n",
    "inputStates = np.array([state1, state2, state3, state4])\n",
    "expectedStates = np.array([state1, state2, state4, state3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pennylane as qml\n",
    "\n",
    "I = np.eye(2)\n",
    "n_qubits = 6\n",
    "size_of_vec = 2**n_qubits\n",
    "num_layers = 7\n",
    "inputStates = np.array([state1, state2, state3, state4])\n",
    "expectedStates = np.array([state1, state2, state4, state3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying ChatPGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "minimize() got an unexpected keyword argument 'disp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m initial_parameters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray\u001b[38;5;241m.\u001b[39mflatten(get_random_weights(num_layers))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Use scipy's minimize function to optimize the parameters\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbasis_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Reshape the optimized parameters back into the matrix form\u001b[39;00m\n\u001b[1;32m    100\u001b[0m optimized_matrix \u001b[38;5;241m=\u001b[39m get_matrix(result\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mreshape((num_layers, \u001b[38;5;241m5\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: minimize() got an unexpected keyword argument 'disp'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pennylane as qml\n",
    "\n",
    "\n",
    "Udot = lambda s1, U, s2 : np.dot(np.conjugate(np.transpose(s1)),np.matmul(U,s2))\n",
    "\n",
    "def nestedKron(*args): # use \"*args\" to access an array of inputs\n",
    "    assert len(args) >= 2\n",
    "    temp = args[0]\n",
    "    for arg in args[1:]:\n",
    "        temp = np.kron(temp, arg)\n",
    "    return temp\n",
    "\n",
    "def get_random_weights(num_layers):\n",
    "    return 2 * np.pi * np.random.random(size=(num_layers, 5)) - np.pi\n",
    "\n",
    "def U_ex(p):\n",
    "    from scipy.linalg import expm\n",
    "    X = [[0,1],[1,0]]\n",
    "    Y = np.array([[0,-1j],[1j,0]], dtype=np.complex128)\n",
    "    Z = [[1,0],[0,-1]]\n",
    "\n",
    "    H_ex = (1/4)*(np.kron(X,X) + np.kron(Y,Y) + np.kron(Z,Z))\n",
    "    # print(f'H_ex.type = {type(H_ex)}')\n",
    "    U_exchange = expm(-1j*p*H_ex) # p is now -pi to pi\n",
    "    return np.array(U_exchange)\n",
    "\n",
    "def single_layer_U(layer_weights):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    firstPart = nestedKron(U_ex(layer_weights[0]), U_ex(layer_weights[1]), U_ex(layer_weights[2]))\n",
    "    secondPart = nestedKron(I, U_ex(layer_weights[3]), U_ex(layer_weights[4]), I)\n",
    "    return np.matmul(secondPart, firstPart)\n",
    "\n",
    "def get_matrix(weights):\n",
    "    totalMatrix = np.eye(size_of_vec)\n",
    "    for layer_weights in weights:\n",
    "        mat = single_layer_U(layer_weights)\n",
    "        totalMatrix = np.matmul(totalMatrix, mat)\n",
    "    return totalMatrix\n",
    "\n",
    "def square_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(expectedStates)):\n",
    "        # c = np.dot(np.conjugate(expectedStates[i]), predictedStates[i])\n",
    "        # c_2 = self.amp_sqrd(c)\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += (1 - fidelity) ** 2\n",
    "    loss /= len(expectedStates)\n",
    "    return 0.5*loss\n",
    "\n",
    "def f_cnot_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(expectedStates)):\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += fidelity\n",
    "    return np.sqrt(1 - (1/4)*abs(loss))\n",
    "\n",
    "# Define the correct operations you want the matrix to perform on basis vectors\n",
    "def target_operations(parameters, inputStates):\n",
    "    # Reshape the parameters into the matrix form\n",
    "    parameters = np.reshape(parameters, (num_layers, 5))\n",
    "    matrix = get_matrix(parameters)\n",
    "\n",
    "    # Perform matrix multiplication with basis vectors\n",
    "    results = []\n",
    "    for i in range(len(inputStates)):\n",
    "        results.append(np.matmul(matrix, inputStates[i]))\n",
    "\n",
    "    # Define the target operations you want (modify this based on your specific task)\n",
    "    target_result = np.array(expectedStates)\n",
    "\n",
    "    # Calculate the loss as the difference between the obtained result and the target result\n",
    "    # loss = square_loss(target_result, results)\n",
    "    loss = f_cnot_loss(target_result, results)\n",
    "    return loss\n",
    "\n",
    "# Example: Set the number of basis vectors and their dimensionality\n",
    "num_vectors = 4\n",
    "vector_dimension = size_of_vec\n",
    "\n",
    "# Generate random basis vectors and target result\n",
    "basis_vectors = np.array(inputStates)\n",
    "target_result = np.array(expectedStates)\n",
    "\n",
    "# Flatten the matrix parameters for optimization\n",
    "initial_parameters = np.ndarray.flatten(get_random_weights(num_layers))\n",
    "\n",
    "# Use scipy's minimize function to optimize the parameters\n",
    "result = minimize(target_operations, initial_parameters, args=(basis_vectors,), method='L-BFGS-B')\n",
    "\n",
    "# Reshape the optimized parameters back into the matrix form\n",
    "optimized_matrix = get_matrix(result.x.reshape((num_layers, 5)))\n",
    "\n",
    "print(\"Optimized Matrix:\")\n",
    "print(optimized_matrix)\n",
    "\n",
    "predStates = [np.matmul(optimized_matrix, mat) for mat in inputStates]\n",
    "print(f\"f_cnot_loss = {f_cnot_loss(expectedStates, predStates)}\")\n",
    "print(f\"square_loss = {square_loss(expectedStates, predStates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def f_cnot_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(np.size(y_true)//(2**n_qubits)):\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += fidelity\n",
    "    return np.sqrt(1 - (1/4)*abs(loss))\n",
    "\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "x_train, y_train = inputStates, expectedStates\n",
    "\n",
    "param_size = num_layers*5\n",
    "parameters = np.array(get_random_weights(num_layers))\n",
    "params = np.ndarray.flatten(parameters).astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build the neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(param_size*2, activation='relu'),\n",
    "    layers.Dense(param_size, activation='tanh'),\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=f_cnot_loss,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(35,) dtype=float32, numpy=\narray([ 2.5269053 ,  0.5212329 ,  0.9739547 ,  2.5969772 ,  0.6503308 ,\n        2.1297295 , -1.931953  ,  0.7041187 , -1.3276408 , -2.2411902 ,\n        1.2720861 ,  1.5847887 ,  3.0150158 ,  1.2192807 , -1.7325749 ,\n       -0.82495993, -1.756478  , -1.1077476 , -2.713036  ,  1.8335059 ,\n        1.614816  ,  2.2740006 ,  2.2506728 , -1.1599189 , -2.0895307 ,\n        3.0720863 , -0.31033638, -0.30215982, -2.318698  , -3.134845  ,\n       -1.3226986 , -2.8566093 ,  1.5320274 ,  0.39243674,  0.13296835],\n      dtype=float32)>),).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(target_operations(parameters, inputVectors))\n\u001b[1;32m     50\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, [parameters])\n\u001b[0;32m---> 51\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1222\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1218\u001b[0m experimental_aggregate_gradients \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperimental_aggregate_gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m )\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m-> 1222\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1184\u001b[0m, in \u001b[0;36mOptimizer.aggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grads_and_vars\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce_sum_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/utils.py:33\u001b[0m, in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all-reduced gradients aggregated via summation.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m  List of (gradient, variable) pairs where gradients have been all-reduced.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(grads_and_vars)\n\u001b[0;32m---> 33\u001b[0m filtered_grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_grads_and_vars:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mstrategy_supports_no_merge_call():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qrl/lib/python3.10/site-packages/keras/src/optimizers/utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[1;32m     76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     82\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m when minimizing the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss. If you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using `model.compile()`, did you forget to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide a `loss` argument?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vars_with_empty_grads]),\n\u001b[1;32m     87\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(35,) dtype=float32, numpy=\narray([ 2.5269053 ,  0.5212329 ,  0.9739547 ,  2.5969772 ,  0.6503308 ,\n        2.1297295 , -1.931953  ,  0.7041187 , -1.3276408 , -2.2411902 ,\n        1.2720861 ,  1.5847887 ,  3.0150158 ,  1.2192807 , -1.7325749 ,\n       -0.82495993, -1.756478  , -1.1077476 , -2.713036  ,  1.8335059 ,\n        1.614816  ,  2.2740006 ,  2.2506728 , -1.1599189 , -2.0895307 ,\n        3.0720863 , -0.31033638, -0.30215982, -2.318698  , -3.134845  ,\n       -1.3226986 , -2.8566093 ,  1.5320274 ,  0.39243674,  0.13296835],\n      dtype=float32)>),)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def f_cnot_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(np.size(y_true)//(2**n_qubits)):\n",
    "        fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "        loss += fidelity\n",
    "    return np.sqrt(1 - (1/4)*abs(loss))\n",
    "\n",
    "\n",
    "# Define the correct operations you want the matrix to perform on basis vectors\n",
    "def target_operations(parameters, inputVectors):\n",
    "    # Perform matrix multiplication with basis vectors\n",
    "    # Reshape the parameters into the matrix form\n",
    "    params = np.reshape(parameters, (num_layers, 5))\n",
    "    matrix = tf.constant(get_matrix(params), dtype=tf.complex128)\n",
    "    # results = tf.matmul(matrix, inputVectors, transpose_b=True)\n",
    "    results = []\n",
    "    for i in range(np.size(inputVectors)//(2**n_qubits)):\n",
    "        results.append(np.matmul(matrix, inputVectors[i]))\n",
    "    results = tf.constant(results, dtype=tf.complex128)\n",
    "\n",
    "    # Define the target operations you want (modify this based on your specific task)\n",
    "    target_results = tf.constant(expectedStates, dtype=tf.complex128)\n",
    "\n",
    "    # Calculate the loss as the mean squared error between the obtained result and the target result\n",
    "    loss = f_cnot_loss(target_results, results)\n",
    "    return loss\n",
    "\n",
    "# Example: Set the number of basis vectors and their dimensionality\n",
    "num_vectors = 4\n",
    "vector_dimension = 2**n_qubits\n",
    "\n",
    "# Generate random basis vectors and target result\n",
    "inputVectors = tf.constant(inputStates, dtype=tf.complex128)\n",
    "\n",
    "# Define the matrix as a TensorFlow Variable\n",
    "parameters = tf.Variable(np.ndarray.flatten(get_random_weights(num_layers)), dtype=tf.float32)\n",
    "\n",
    "# Use an optimizer to minimize the loss\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.constant(target_operations(parameters, inputVectors))\n",
    "\n",
    "    gradients = tape.gradient(loss, [parameters])\n",
    "    optimizer.apply_gradients(zip(gradients, [parameters]))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Get the optimized matrix\n",
    "parameters = np.reshape(parameters, (num_layers, 5))\n",
    "matrix = get_matrix(parameters)\n",
    "optimized_matrix = matrix.numpy()\n",
    "\n",
    "print(\"Optimized Matrix:\")\n",
    "print(optimized_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml\n",
    "\n",
    "class MatrixOperations():\n",
    "    def __init__(self, expectedStates, inputStates, weights, num_layers):\n",
    "        self.expectedStates = expectedStates\n",
    "        self.inputStates = inputStates\n",
    "        self.weights = weights\n",
    "        self.num_layers = num_layers\n",
    "        self.Udot = lambda s1, U, s2 : np.dot(np.conjugate(np.transpose(s1)),np.matmul(U,s2))\n",
    "\n",
    "    def nestedKron(self, *args): # use \"*args\" to access an array of inputs\n",
    "        assert len(args) >= 2\n",
    "        temp = args[0]\n",
    "        for arg in args[1:]:\n",
    "            temp = np.kron(temp, arg)\n",
    "        return temp\n",
    "\n",
    "    def get_random_weights(self):\n",
    "        return 2 * np.random.random(size=(self.num_layers, 5)) - 1\n",
    "\n",
    "    def U_ex(self, p):\n",
    "        from scipy.linalg import expm\n",
    "        X = [[0,1],[1,0]]\n",
    "        Y = np.array([[0,-1j],[1j,0]], dtype=np.complex128)\n",
    "        Z = [[1,0],[0,-1]]\n",
    "\n",
    "        H_ex = (1/4)*(np.kron(X,X) + np.kron(Y,Y) + np.kron(Z,Z))\n",
    "        # print(f'H_ex.type = {type(H_ex)}')\n",
    "        U_exchange = expm(-1j*np.pi*p*H_ex) # p is -1 to 1\n",
    "        return np.array(U_exchange)\n",
    "\n",
    "    def get_predictions(self, weights):\n",
    "        matOp = self.get_total_matrix(weights)\n",
    "        results = []\n",
    "        for i in range(len(self.inputStates)):\n",
    "            results.append(np.matmul(matOp, self.inputStates[i]))\n",
    "        return np.array(results)\n",
    "    \n",
    "    def count_weight_vals_that_are_zero(self, weights, movement):\n",
    "        constraint = lambda val : val >= -movement*1.5 and val <= movement*1.5\n",
    "\n",
    "        count = 0\n",
    "        for layer in weights:\n",
    "            for block_val in layer:\n",
    "                count += 1 if constraint(block_val) else 0\n",
    "        return count\n",
    "\n",
    "\n",
    "    def single_layer_U(self,layer_weights):\n",
    "        \"\"\"Trainable circuit block.\"\"\"\n",
    "        firstPart = self.nestedKron(self.U_ex(layer_weights[0]), self.U_ex(layer_weights[1]), self.U_ex(layer_weights[2]))\n",
    "        secondPart = self.nestedKron(I, self.U_ex(layer_weights[3]), self.U_ex(layer_weights[4]), I)\n",
    "        return np.matmul(secondPart, firstPart)\n",
    "\n",
    "    def get_total_matrix(self,weights):\n",
    "        totalMatrix = np.eye(size_of_vec)\n",
    "        for layer_weights in weights:\n",
    "            mat = self.single_layer_U(layer_weights)\n",
    "            totalMatrix = np.matmul(totalMatrix, mat)\n",
    "        return totalMatrix\n",
    "\n",
    "    def f_cnot_loss(self,y_true, y_pred):\n",
    "        loss = 0\n",
    "        for i in range(len(expectedStates)):\n",
    "            fidelity = qml.math.fidelity_statevector(y_true[i], y_pred[i])\n",
    "            loss += fidelity\n",
    "        return np.sqrt(1 - (1/4)*abs(loss))\n",
    "    \n",
    "    def cost_fn(self, weights):\n",
    "        preds = self.get_predictions(weights)\n",
    "        loss = self.f_cnot_loss(self.expectedStates, preds)\n",
    "        return loss\n",
    "\n",
    "    # Define the correct operations you want the matrix to perform on basis vectors\n",
    "    def target_operations(self,parameters):\n",
    "        # Reshape the parameters into the matrix form\n",
    "        parameters = np.reshape(parameters, (self.num_layers, 5))\n",
    "        matrix = self.get_total_matrix(parameters)\n",
    "\n",
    "        # Perform matrix multiplication with basis vectors\n",
    "        results = self.get_predictions(parameters)\n",
    "\n",
    "        # Define the target operations you want (modify this based on your specific task)\n",
    "        target_result = np.array(self.expectedStates)\n",
    "\n",
    "        # Calculate the loss as the difference between the obtained result and the target result\n",
    "        # loss = square_loss(target_result, results)\n",
    "        loss = f_cnot_loss(target_result, results)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3050.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "num_layers=7\n",
    "action_space = gym.spaces.Box(low=-1, high=1, shape=(num_layers,5,))\n",
    "\n",
    "state = 2*np.random.random(size=(7,5))-1\n",
    "state += action_space.sample()\n",
    "size = lambda boxSpace : boxSpace.shape[0] * boxSpace.shape[1]\n",
    "size(action_space), np.array([1,2]).size\n",
    "import math\n",
    "def computeRewardAndUpdateState():\n",
    "    cond = lambda loss : True if loss<0.1 else False\n",
    "    getBoost = lambda loss : int(math.ceil(-math.log2(loss**3))*15) if cond(loss) else 0\n",
    "    getExtraZeroGateBoost = lambda loss, zero_counts : int(math.ceil(-math.log2((1/zero_counts)**10)))*10 if zero_counts>=1 and cond(loss) else zero_counts*5\n",
    "    loss = 1e-17\n",
    "    zero_counts = 0\n",
    "    totalReward = (1-loss)*500 + getBoost(loss) + getExtraZeroGateBoost(loss, zero_counts)\n",
    "\n",
    "    return totalReward\n",
    "\n",
    "computeRewardAndUpdateState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from reinforcement_learning.Env import QuantumEnvironment\n",
    "\n",
    "# Define the reinforcement learning model using a simple neural network\n",
    "def build_model(input_dim, output_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_dim, activation='tanh')  # Use tanh to ensure output in the range [-1, 1]\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the Deep Deterministic Policy Gradient (DDPG) algorithm\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = build_model(state_dim, action_dim) # this only gives best action to take based on state as input\n",
    "        self.critic = build_model(state_dim + action_dim, 1) # this takes both state and action from actor to evaluate how good it is\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# guassian noise needed for explore-explot dilemma -----------------------------\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.actor(state.reshape(1, -1)).numpy().flatten()\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states):\n",
    "        # Define the actor and critic training steps using the DDPG algorithm\n",
    "        pass\n",
    "# squared bellman error loss -----------------------\n",
    "# Hyperparameters\n",
    "num_parameters = 10\n",
    "state_dim = 4  # Adjust based on your state representation\n",
    "action_dim = num_parameters\n",
    "\n",
    "# Initialize quantum environment and DDPG agent\n",
    "env = QuantumEnvironment(num_parameters)\n",
    "agent = DDPGAgent(state_dim, action_dim)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        agent.train(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Env' object has no attribute 'action_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mEnv()\n\u001b[0;32m----> 3\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Env' object has no attribute 'action_space'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.Env()\n",
    "env.action_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square loss = 0.14172274545673982\n"
     ]
    }
   ],
   "source": [
    "p1 = np.arccos(-1/np.sqrt(3))/np.pi\n",
    "p2 = np.arcsin(1/3)/np.pi\n",
    "\n",
    "\n",
    "def Id_n(n):\n",
    "    assert n >= 0\n",
    "    if n==0:\n",
    "        return 1\n",
    "    temp = I\n",
    "    for i in range(n-1):\n",
    "        temp = np.kron(temp, I)\n",
    "    return temp\n",
    "\n",
    "\n",
    "bounds = [[3,4],[2,5],[3,4],[2,5],[1,4],[2,5],[1,4],[2,5],[1,4],[2,5],[3,4],[2,5],[3,4]]\n",
    "operators = [U_ex(p1),\n",
    "np.kron(U_ex(1/2),U_ex(p2)),\n",
    "U_ex(1),\n",
    "np.kron(U_ex(-1/2),U_ex(-1/2)),\n",
    "np.kron(U_ex(1),U_ex(-1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(1)),\n",
    "np.kron(U_ex(-1/2),U_ex(1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(1)),\n",
    "np.kron(U_ex(1),U_ex(-1/2)),\n",
    "np.kron(U_ex(-1/2),U_ex(-1/2)),\n",
    "U_ex(1),\n",
    "np.kron(U_ex(1/2),U_ex(1-p2)),\n",
    "U_ex(-p1)]\n",
    "\n",
    "\n",
    "newOps = []\n",
    "for i, (start, end) in enumerate(bounds):\n",
    "    temp = nestedKron(Id_n(start),operators[i], Id_n(5-end))\n",
    "    newOps.append(temp.copy())\n",
    "\n",
    "totalOperator = np.eye(2**6)\n",
    "for op in newOps:\n",
    "    totalOperator = np.matmul(op,totalOperator)\n",
    "\n",
    "U_cnot = totalOperator.copy()\n",
    "\n",
    "\n",
    "predStates = [np.matmul(U_cnot, mat) for mat in inputStates]\n",
    "print(f\"square loss = {square_loss(expectedStates, predStates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from the pennylane qml model is = 0.3678316991400055\n"
     ]
    }
   ],
   "source": [
    "from QNN import Ansatz\n",
    "ansatz = Ansatz(num_layers=num_layers)\n",
    "ansatz.weights = result.x.reshape((num_layers, 5))\n",
    "predStates = ansatz.get_predictions(ansatz.weights, inputStates)\n",
    "loss = ansatz.square_loss(expectedStates, predStates)\n",
    "\n",
    "print(f'loss from the pennylane qml model is = {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
